# RAG Chatbot: Document-Based Q&A System

## Overview

This project is a smart chatbot that answers user queries based only on a specific PDF document (like Terms & Conditions). It uses **Retrieval-Augmented Generation (RAG)**, combining semantic search (via embeddings and FAISS vector database) with a large language model (LLM) to deliver accurate, transparent, and grounded answers through a real-time Streamlit web interface.

---

## Features

- **Document-grounded Q&A:** Answers come strictly from your uploaded document(s), no outside data.
- **Semantic search:** Uses pre-trained embeddings (e.g., all-MiniLM-L6-v2) and a vector database (FAISS) for highly relevant text retrieval.
- **LLM-powered generation:** Utilizes models like Gemma, Mistral, or Llama (via Groq API) to generate natural, context-aware responses.
- **Real-time chat:** Streamlit web app streams responses as they are generated, for a conversational, interactive experience.
- **Transparency:** Highlights which chunks of the document were used for each answer.

---

## Project Structure

â”œâ”€â”€ data_cleaning.ipynb       # Cleans and chunks the PDF document text
â”œâ”€â”€ embedding.ipynb           # Converts text chunks to vector embeddings and builds vector DB (FAISS)
â”œâ”€â”€ generator.py              # Connects to LLM (via Groq) for grounded answer generation
â”œâ”€â”€ pipeline.py               # Runs retrieval, prompt construction, and answer generation pipeline
â”œâ”€â”€ app.py                    # Streamlit app with live chat UI
â””â”€â”€ AI_Training_Document.pdf  # Example PDF document (upload your own as needed)




---

## ðŸ”¬ How It Works (Deep Dive)

1. **Chunking:** Splits PDF into small, language-aware segments for fine-grained retrieval.
2. **Embedding:** Uses transformer models to convert each chunk into a high-dimensional vector representing its meaning.
3. **Indexing:** Embedding vectors are stored in FAISS for fast similarity search.
4. **Retrieval:** On user query, the system computes the queryâ€™s embedding and retrieves the k-most relevant chunks from FAISS.
5. **Prompt Construction:** Gathers the query and retrieved chunks into a formatted prompt template.
6. **LLM Generation:** Sends the prompt to the selected LLM API; answer is generated â€˜groundedâ€™ in the retrieved evidence only.
7. **UI:** Streams answer to web chat, highlighting which document chunks were referenced for maximum transparency.

---

## ðŸ§© Example Usage

**User:** â€œWhat penalties are there for late payment?â€
- The app finds related regulations in your document, sends them to the LLM, and immediately streams an answerâ€”**citing the exact paragraphs used.**

---

## ðŸ“ˆ Extending & Customizing

- **Support for multi-PDF corpora:** Track documents and chunk sources, concatenate multiple embeddings, or shard by document domain.
- **Swap LLM APIs:** Abstract the `generator.py` logic to use OpenAI, Anthropic, or local LLMs.
- **Embed better:** Swap out embedding models for domain-tuned ones.
- **Improve prompt engineering:** Modify the template for greater control or to add citation formatting.
- **Frontend tweaks:** Refine Streamlitâ€™s UI/UX or migrate to React for more features.

---

## ðŸ›¡ï¸ Security & Privacy

- All documents and chats stay on your own infrastructure.
- (Optional) Add user authentication or data redaction per organization policy.

---

## ðŸ§ª Testing & Evaluation

- Includes sample queries in `tests/` (add as needed).
- Use retrieval and answer correctness metrics (accuracy, recall, response groundedness).
- Add end-to-end integration tests with Streamlitâ€™s testing tools.

---

## ðŸ“š Dependencies

- python >= 3.9
- streamlit
- sentence-transformers
- faiss-cpu
- pypdf / pdfplumber
- requests
- (plus LLM API dependencies, e.g., groq, openai)

---


